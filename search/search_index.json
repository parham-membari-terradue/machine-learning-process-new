{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Event-Driven Water Bodies Detection Using Argo Workflows and Argo Events","text":""},{"location":"#introduction","title":"Introduction","text":"<p>This learning resource demonstrates a machine learning system for classification of Sentinel-2 images into 10 different classes using cloud-native technologies. The system leverages MLFLOW to track the training process and select the best candidate trained model from MLFLOW server. </p> <p>There are two workflows developed one for training a deep learning model classifier on EuroSAT dataset and one for running prediction on a real world Sentinel-2 data.  The automation is achieved using Kubernetes-native tools, making the setup scalable, modular, and suitable for Earth observation and geospatial applications.</p>"},{"location":"#key-components","title":"Key Components","text":"<p>This setup integrates the following technologies and concepts:</p>"},{"location":"#mlflow","title":"MLFLOW","text":"<ul> <li>Manage end-to-end ML workflows, from development to production</li> <li>End-to-end MLOps solution for traditional ML, including integrations with traditional ML models, and Deep learning one.</li> <li>Simple, low-code performance tracking with autologging</li> <li>State-of-the-art UI for model analysis and comparison</li> </ul>"},{"location":"#stac-endpoint","title":"STAC Endpoint","text":"<ul> <li>Serves as the primary data source, providing geospatial data in a standardized format.</li> </ul>"},{"location":"#high-level-architecture","title":"High-Level Architecture","text":"<p>The system is designed to handle the following flow:</p> <ol> <li> <p>Training pipeline: A CNN model trained on EuroSAT dataset which already exist on a dedicated STAC endpoint. The MLFLOW track the whole process to monitor the life cycle of training.</p> </li> <li> <p>Inference: Run the inference pipeline to perform tile-based classification on Sentinel-2 L1C products.</p> </li> <li> <p>Workflow Execution: Both training and inference pipeline will be executed using the CWL-based algorithm.</p> </li> </ol>"},{"location":"#why-use-this-setup","title":"Why Use This Setup?","text":"<p>This setup demonstrates the power of combining machine learning paradigms with container-native workflows to enable scalable geospatial analysis.</p> <p>It is particularly suited for Earth observation and scientific workflows because:</p> <ul> <li>Scalability: Kubernetes ensures workflows can handle varying loads effectively.</li> <li>Modularity: Components can be easily reused or replaced for other applications.</li> <li>Automation: Events trigger workflows without manual intervention, enabling real-time processing.</li> </ul> <p>Through this resource, you'll learn to implement a cloud-native pipeline for tile-based classification, which can be extended to other geospatial or scientific applications.</p>"},{"location":"inference-container/","title":"Inference container:","text":"<p>This module helps the user to create a pipeline for inference, which is responsible for reading assets from a Sentinel-2 L1C STAC item, loading 13 common bands, and providing a binary mask TIFF image using a CNN model that has already been trained (Please check the training-container). </p>"},{"location":"inference-container/#make-inference-module","title":"Make Inference Module:","text":"<p>Inputs: - <code>input_reference</code>: The reference to a staged-in Sentinel-2 L1C product to run the inference on Sentinel-2 L1C images with 13 common bands.</p> <p>Outputs:</p> <ul> <li><code>{STAC_ITEM_ID}_classified.tif</code>: A binary <code>.tif</code> image in <code>COG</code> format classifies:</li> </ul> Class ID Class Name 0 AnnualCrop 1 Forest 2 HerbaceousVegetation 3 Highway 4 Industrial 5 Pasture 6 PermanentCrop 7 Residential 8 River 9 SeaLake 10 No Data <ul> <li><code>overview_{STAC_ITEM_ID}_classified.tif</code>: A binary <code>.tif</code> image in <code>COG</code> format classifies:</li> </ul> Class ID Class Name 0 AnnualCrop 1 Forest 2 HerbaceousVegetation 3 Highway 4 Industrial 5 Pasture 6 PermanentCrop 7 Residential 8 River 9 SeaLake 10 No Data <ul> <li><code>STAC objects</code>: STAC objects related to the provided masks, including STAC catalog and STAC Item.</li> </ul> <p>Certainly! Here's a grammatically correct and technically clearer version of your section:</p>"},{"location":"inference-container/#how-the-application-package-works","title":"How the Application Package Works","text":"<p>Before developing the inference module, one crucial step must be completed: the user needs to select a candidate model from MLflow, based on preferred evaluation metrics. The selected model should then be exported in the ONNX format and used for building the inference module.</p> <p>The user must also provide a staged-in Sentinel-2 L1C product and read the STAC Item's 13 common spectral bands, which serve as the input to the trained model. These assets are processed using a 64\u00d764 sliding window to handle memory limitations and ensure the input size matches that of the machine learning model.</p> <p>Finally, the model generates predictions, and the resulting classification masks are stored as a COG (Cloud-Optimized GeoTIFF) image.</p>"},{"location":"inference-cwl/","title":"Inference Module &amp; CWL Runner","text":"<p>In the training mdule, the user trained a CNN model on EuroSAT dataset to classify image chips into 10 different classes and tracked the workflow with MLFlow.</p> <p>In this Application Package, the user provide a cwl document to performs inference by applying the trained model to unseen data to generate a classified image. The cwl document containing one main workflow to execute one CommandLineTool step:</p> <p>The Application Package takes as input a staged-in Sentinel-2 L1C data and classifies it into 11 land cover classes:</p> Class ID Class Name 0 AnnualCrop 1 Forest 2 HerbaceousVegetation 3 Highway 4 Industrial 5 Pasture 6 PermanentCrop 7 Residential 8 River 9 SeaLake 10 No Data <p>For executing the application package, the user have the options to use whether cwltool or calrissian.</p>"},{"location":"inference-cwl/#inputs","title":"Inputs","text":"<p>The CWL file takes as input a reference to a directory containing the staged Sentinel-2 L1C product.</p>"},{"location":"inference-cwl/#how-to-execute-the-application-package","title":"How to Execute the Application Package?","text":"<p>Before executing the application package with a CWL runner, the user must first stage in the Sentinel-2 L1C data. Instructions for doing this can be found in the stage-in guide. Then update the latest docker image reference in the cwl file as below: <pre><code>cd inference/app-package\nVERSION=\"0.0.2\"\ncurl -L -o \"tile-sat-inference.${VERSION}.cwl\" \\\n  \"https://github.com/parham-membari-terradue/machine-learning-process-new/releases/download/${VERSION}/tile-sat-inference.${VERSION}.cwl\"\n</code></pre></p>"},{"location":"inference-cwl/#run-the-application-package","title":"Run the Application Package:","text":"<p>There are two methods to execute the application:</p> <ul> <li> <p>Executing the <code>tile-sat-inference</code> app using <code>cwltool</code> in a terminal:</p> <pre><code>cwltool --podman --debug tile-sat-inference.cwl#tile-sat-inference params.yml\n</code></pre> </li> <li> <p>Executing the water-bodies-app using <code>calrissian</code> in a terminal:</p> <pre><code>calrissian --debug --stdout /calrissian/out.json --stderr /calrissian/stderr.log --usage-report /calrissian/report.json --max-ram 10G --max-cores 2 --tmp-outdir-prefix /calrissian/tmp/ --outdir /calrissian/results/ --tool-logs-basepath /calrissian/logs tile-sat-inference.cwl#tile-sat-inference params.yml\n</code></pre> </li> </ul>"},{"location":"inference-cwl/#how-the-cwl-document-works","title":"How the CWL Document Works:","text":"<p>The CWL file can be triggered using <code>cwltool</code> or <code>calrissian</code>. The user provides a <code>params.yml</code> file that passes all inputs needed by the CWL file to execute the module. The CWL file is designed to execute the module based on the structure below:</p> <p></p> <p><code>[]</code> in the image above indicates that the user may pass a list of parameters to the application package.</p> <p>The Application Package will generate a list of directories containing intermediate or final output. The number of folders containing a <code>{STAC_ITEM_ID}_classified.tif</code> and the corresponding STAC objects, such as STAC Catalog and STAC Item, depends on the number of input Sentinel-2 items.</p>"},{"location":"insights/","title":"Lessons Learned from Building a Machine Learning process for Geospatial Data","text":""},{"location":"insights/#introduction","title":"Introduction","text":"<p>This page highlights the technical challenges, design decisions, and key insights gained while developing the machine learning process for geospatial data pipeline. </p> <p>It also includes recommendations for future improvements and practical advice for replicating or extending the setup.</p>"},{"location":"insights/#design-decisions","title":"Design Decisions","text":""},{"location":"insights/#modular-workflow-templates","title":"Modular Workflow Templates","text":"<p>Decision: Separate the CWL execution, training pipeline, and inference pipeline  into distinct workflow templates.</p> <p>Outcome: * Enhanced reusability for other geospatial pipelines requiring similar preprocessing steps.</p>"},{"location":"insights/#stac-integration","title":"STAC Integration","text":"<p>Decision: Leverage the STAC API, Geoparquet, and DuckDB for querying and storing geospatial data.</p> <p>Outcome: * Improved interoperability with other geospatial tools and standards.</p>"},{"location":"insights/#tracking-the-process","title":"Tracking the process","text":"<p>Decision: Use MLFLOW exclusively for tracking the process of training workflow and selecting the best model candidate.</p>"},{"location":"insights/#challenges-and-solutions","title":"Challenges and Solutions","text":""},{"location":"insights/#build-docker-images","title":"Build Docker Images","text":"<p>Challenge: Initially, we used an advanced tooling technique that leveraged Taskfile to build a Kaniko-based image and reference the CWL files. The image was then pushed to ttl.sh, a temporary image registry. This will help us to execute the application packages using calrissian. However, this process was slow and hard to debug, often failing due to the large size of the Kaniko images.</p> <p>Solution: We now push the Docker images to a dedicated GitHub Container Registry.</p>"},{"location":"packages/","title":"Application Packages","text":"<p>This tutorial provides two separate application packages:</p> <ul> <li><code>training</code> </li> <li><code>inference</code></li> </ul> <p>Each application package has its own Docker image, which has been published to a dedicated GitHub Container Registry.</p> <p>For more details on how each package works, refer to the documentation for training and inference.</p>"},{"location":"stage-in/","title":"Stage-in","text":"<p>To test the inference module, the user must first stage in Sentinel-2 L1C data from Copernicus Data Space.</p> <p>This is done by running a Bash script that uses the <code>Stars</code> tool to download and prepare the data. For more information on how <code>Stars</code> works, refer to its official documentation.</p>"},{"location":"training-container/","title":"Training a Machine Learning Model- Container","text":"<p>This tutorial containing a python application for training a deep learning model on EuroSAT dataset for tile-based classification task and employs MLflow for monitoring the ML model development cycle. MLflow is a crucial tool that ensures effective log tracking and preserves key information, including specific code versions, datasets used, and model hyperparameters. By logging this information, the reproducibility of the work drastically increases, enabling users to revisit and replicate past experiments accurately. Moreover, quality metrics such as classification accuracy, loss function fluctuations, and inference time are also tracked, enabling easy comparison between different models. The dataset used in this project consists of Sentinel-2 satellite images labeled with corresponding land use and cover categories. It provides a comprehensive representation of various land features. The dataset comprises 27,000 labeled and geo-referenced images, divided into 10 distinct classes. The multi-spectral version of the dataset includes all 13 Sentinel-2 bands, which retains the original value range of the Sentinel-2 bands, enabling access to a more comprehensive set of spectral information. You can find the dataset on a dedicated STAC endpoint. </p> <p></p>"},{"location":"training-container/#inputs","title":"Inputs","text":"<p>The application has the option to train the model using CPU or GPU to accelerate the training process. It received a set of input parameters including: <pre><code>stac_endpoint_url: https://ai-extensions-stac.terradue.com/collections/Euro_SAT\nBATCH_SIZE: 4\nEPOCHS: 3\nLEARNING_RATE: 0.0001\nDECAY: 0.1  ### float\nEPSILON: 0.000002\nMEMENTUM: 0.95\n# choose one of binary_crossentropy/cosine_similarity/mean_absolute_error/mean_squared_logarithmic_error\n# squared_hinge\nLOSS: categorical_crossentropy  \n# choose one of  l1,l2,None\nREGULIZER: None\n# try Adam/SGD/RMSprop\nOPTIMIZER: Adam\n###############################################################\n###############################################################\n# Dataset\nSAMPLES_PER_CLASS: 10\nCLASSES: 10\nIMAGE_SIZE: [64, 64, 13]\nenable_data_ingestion: True\n</code></pre> and some environment variable must be set:</p> <pre><code># Environment variables\n# AWS\nAWS_S3_ENDPOINT: #AWS_S3_ENDPOINT \nAWS_REGION: fr-par # AWS_REGION \nAWS_DEFAULT_REGION: fr-par #AWS_DEFAULT_REGION \nAWS_ACCESS_KEY_ID:  #AWS_ACCESS_KEY_ID \nAWS_SECRET_ACCESS_KEY:  # AWS_SECRET_ACCESS_KEY \nBUCKET_NAME: ai-ext-bucket-dev # BUCKET_NAME ai-ext-bucket-dev\n##############################################################\n##############################################################\n# STAC\nIAM_URL:  # IAM_URL\nIAM_PASSWORD:  #IAM_PASSWORD\nMLFLOW_TRACKING_URI: http://my-mlflow:5000\n</code></pre>"},{"location":"training-container/#how-the-application-structured-internally","title":"How the application structured internally","text":"<p>The training pipeline for developing the training module encompasses 4 main components including: - Data Ingestion - Based Model Architecture - Training - Evaluation</p> <p>The pipeline for this task is illustrated in the diagram below:</p> <p></p>"},{"location":"training-container/#data-ingestion","title":"Data Ingestion","text":"<p>This component is designed to fetch data from a dedicated STAC endpoint containing a collection of STAC Items representing EuroSAT image chips. The user can query the collection using DuckDB on a GeoParquet file and split the resulting data into training, validation, and test datasets.</p>"},{"location":"training-container/#based-model-architecture","title":"Based Model Architecture","text":"<p>In this component, the user will design a CNN based model with 7 layers. The first layer serves as the input layer, accepting an image with a shape of (13, 64, 64) or any other cubic image shapes (e.g. (3,64,64)). This is followed by 4 convolutional layers, each employing a relu activation function, a BatchNormalization layer,a 2D MaxPooling operation, and a Dropout layer. Subsequently, the model includes a Dense layer, and finally, the output layer generates a vector with 10 cells. Notably, the output layer utilizes the softmax activation function to produce the probabilities associated with each class. The user will choose a loss function, and an optimizer among available loss functions and optimizers. Eventually, the model is compiled and located under <code>output/prepare_based_model</code>.</p>"},{"location":"training-container/#training","title":"Training","text":"<p>This component is responsible for training the model for a specified number of epochs, as provided by the user through the application package inputs.</p>"},{"location":"training-container/#evaluation","title":"Evaluation","text":"<p>The user can evaluate the trained model, and MLflow will track the process for each run under a designated experiment. Once the MLflow service is deployed and running on port <code>5000</code>, the UI can be accessed at http://localhost:5000.</p> <p>MLflow tracks the following:</p> <ul> <li>Evaluation metrics, including <code>Accuracy</code>, <code>Precision</code>, <code>Recall</code>, and the loss value  </li> <li>Trained machine learning model saved after each run  </li> <li>Additional artifacts, such as:</li> <li>Loss curve plot during training  </li> <li>Confusion matrix</li> </ul>"},{"location":"training-container/#for-developers","title":"For developers","text":"<ol> <li> <p><code>src</code>/ <code>tile_based_training</code> /</p> <ul> <li>components /<ul> <li>Containing all components such as data_ingestion.py, prepare_base_model.py, train_model.py , model_evaluation.py, inference.py.</li> </ul> </li> <li>config /<ul> <li>Containing all configuration needed for each component.</li> </ul> </li> <li>utils /<ul> <li>to define helper functions.</li> </ul> </li> <li>pipeline /<ul> <li>to define the order of executing for each component.</li> </ul> </li> </ul> <p>Notice: For more information how above units work please check the notebook under <code>trials</code> directory. 2. <code>output</code>/: A folder where all intermediate artifacts like refrences to train and test data, models, etc will be saved here. 3. <code>config</code>/ : A folder containing all configuration needed for the application is stored such as paths, name of classes , etc.  4. <code>pyproject.toml</code>: installing all dependencies in hatch envronment.</p> </li> </ol>"},{"location":"training-cwl/","title":"Training Module &amp; CWL Runner","text":"<p>The users have the option to execute their training remotely on a remote machine, which becomes their preferred choice for conducting long-running experiments.   The user will be able to train a simple training job and track evaluation metrics, model's hyper-parameters, application inputs, and artifacts using mlflow. He/She should build a docker image from the application package which is explained in training-container and push to a dedicated image registry (in this case we use github container registry). Then, The user develop a cwl document with containing a top layer workflow with a CommandLineTool step which is responsible for executing the training pipeline. For executing the application package, the user have the options to use whether cwltool or calrissian.</p>"},{"location":"training-cwl/#how-to-execute-the-application-package","title":"How to execute the application-package?","text":"<p>Before executing the application package with a CWL runner, the user must first update the params.yaml including the inputs below:</p> <p>with the correct credentials as well as updating the latest docker image reference in the cwl file as below: <pre><code>stac_endpoint_url: https://ai-extensions-stac.terradue.com/collections/Euro_SAT\nBATCH_SIZE: 4\nEPOCHS: 3\nLEARNING_RATE: 0.0001\nDECAY: 0.1  ### float\nEPSILON: 0.000002\nMEMENTUM: 0.95\n# choose one of binary_crossentropy/cosine_similarity/mean_absolute_error/mean_squared_logarithmic_error\n# squared_hinge\nLOSS: categorical_crossentropy  \n# choose one of  l1,l2,None\nREGULIZER: None\n# try Adam/SGD/RMSprop\nOPTIMIZER: Adam\n###############################################################\n###############################################################\n# Dataset\nSAMPLES_PER_CLASS: 10\nCLASSES: 10\nIMAGE_SIZE: [64, 64, 13]\nenable_data_ingestion: True\n</code></pre> and some environment variable must be set:</p> <p><pre><code># Environment variables\n# AWS\nAWS_S3_ENDPOINT: #AWS_S3_ENDPOINT \nAWS_REGION: fr-par # AWS_REGION \nAWS_DEFAULT_REGION: fr-par #AWS_DEFAULT_REGION \nAWS_ACCESS_KEY_ID:  #AWS_ACCESS_KEY_ID \nAWS_SECRET_ACCESS_KEY:  # AWS_SECRET_ACCESS_KEY \nBUCKET_NAME: ai-ext-bucket-dev # BUCKET_NAME ai-ext-bucket-dev\n##############################################################\n##############################################################\n# STAC\nIAM_URL:  # IAM_URL\nIAM_PASSWORD:  #IAM_PASSWORD\nMLFLOW_TRACKING_URI: http://my-mlflow:5000\n</code></pre> The latest version of cwl document can be fetched as below:</p> <pre><code>cd training/app-package\nVERSION=\"0.0.2\"\ncurl -L -o \"tile-sat-training.${VERSION}.cwl\" \\\n  \"https://github.com/parham-membari-terradue/machine-learning-process-new/releases/download/${VERSION}/tile-sat-training.${VERSION}.cwl\"\n</code></pre>"},{"location":"training-cwl/#run-the-application-package","title":"Run the Application package:","text":"<p>There are two methods to execute the application:</p> <ul> <li> <p>Executing the tile-based-training using cwltool in a terminal:</p> <pre><code>cwltool --podman --debug tile-sat-training.cwl#tile-sat-training params.yml\n</code></pre> </li> <li> <p>Executing the tile-based classification using calrissian in a terminal:</p> <pre><code>calrissian --debug --stdout /calrissian/out.json --stderr /calrissian/stderr.log --usage-report /calrissian/report.json --max-ram 10G --max-cores 2 --tmp-outdir-prefix /calrissian/tmp/ --outdir /calrissian/results/ --tool-logs-basepath /calrissian/logs tile-sat-training.cwl#tile-sat-training params.yaml\n</code></pre> </li> </ul>"},{"location":"training-cwl/#additional-resources","title":"Additional Resources","text":"<p>Please see below for additional resources and tutorials for developing Earth Observation (EO) Application Packages using the CWL: * Application Package and CWL as a solution for EO portability * A simple EO Application Package for getting started * Mastering EO Application Packaging with CWL * Inference with the EO Application Package</p>"}]}